{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dfed30b6-629c-440c-82b8-5c25c47de415",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4afa3e07-be89-40fd-8f6d-068835f9b214",
   "metadata": {},
   "outputs": [],
   "source": [
    "import medmnist\n",
    "from medmnist import INFO, Evaluator, BloodMNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0e5cd48-4773-4e52-ad7e-442f266e1d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = BloodMNIST(split=\"train\", size=64, download=True)\n",
    "val_dataset = BloodMNIST(split=\"val\", size=64, download=True)\n",
    "test_dataset = BloodMNIST(split=\"test\", size=64, download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca28faf1-badb-4063-84d4-5b73c7b866a8",
   "metadata": {},
   "source": [
    "## Dinobloom\n",
    "\n",
    "Let's test the embeddings produced by the Dinobloom model.  \n",
    "The [small version](https://huggingface.co/1aurent/vit_small_patch14_224.dinobloom)\n",
    "will do the work for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "edf9b1d9-6aad-41d5-ab55-3d455b0a5c13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "396b53d817814afbb6c7b00ea61d0148",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/613 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8a4b581360b4d008fad3d10fa84ed7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/86.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import timm\n",
    "\n",
    "# load model from the hub\n",
    "model = timm.create_model(\n",
    "  model_name=\"hf-hub:1aurent/vit_small_patch14_224.dinobloom\",\n",
    "  pretrained=True,\n",
    ").eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2014afca-0a55-4619-b28e-e8c73cd177aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get model specific transforms (normalization, resize)\n",
    "data_config = timm.data.resolve_model_data_config(model)\n",
    "transforms = timm.data.create_transform(**data_config, is_training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a19140cd-407f-4d07-bf4e-cc302ec652a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torch.utils import data\n",
    "\n",
    "class CustomDataset(data.Dataset):\n",
    "\n",
    "    def __init__(self, data, transform=None):\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img, label = self.data[index]\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return (img, label)\n",
    "\n",
    "\n",
    "class EmbeddingDataset(data.Dataset):\n",
    "    def __init__(self, dataset, model, transform, device):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.embeddings, self.labels = self._create_vectors(model, dataset)\n",
    "\n",
    "    def _create_vectors(self, model, dataset):\n",
    "        embeddings = []\n",
    "        label_list = []\n",
    "        model.to(self.device)\n",
    "        dataloader = data.DataLoader(dataset, batch_size=8)\n",
    "\n",
    "        # loop over the data\n",
    "        for batch in tqdm(dataloader):\n",
    "            images, labels = batch\n",
    "            images = images.to(self.device)\n",
    "\n",
    "            # make the embeddings from the batch\n",
    "            with torch.no_grad():\n",
    "                embs = model(images).to(\"cpu\")\n",
    "            \n",
    "            embeddings.append(embs)\n",
    "            label_list.append(labels)\n",
    "\n",
    "\n",
    "        embeddings = torch.cat(embeddings, dim=0)\n",
    "        label_list = torch.cat(label_list, dim=0)\n",
    "        return embeddings, label_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.embeddings)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.embeddings[idx, :], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8798ad06-3bd8-46dc-8b79-43bab2a800e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1495/1495 [06:01<00:00,  4.14it/s]\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda:6\"\n",
    "training_dataset = CustomDataset(train_dataset, transforms)\n",
    "emb_train = EmbeddingDataset(training_dataset, model, transform, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9329911e-81a3-4c5e-91fc-ab9242c31191",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 214/214 [00:27<00:00,  7.66it/s]\n"
     ]
    }
   ],
   "source": [
    "validation_dataset = CustomDataset(val_dataset, transforms)\n",
    "emb_validation = EmbeddingDataset(validation_dataset, model, transform, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ebed1467-2c9b-4081-afaa-12fa75fd3ac4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([384])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor, label = emb_train[0]\n",
    "tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ae8fc09b-7629-4975-aff1-8490a77e9367",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guevel/.conda/envs/cell_sim/lib/python3.10/site-packages/sklearn/utils/validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/guevel/.conda/envs/cell_sim/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear probe accuracy: 0.9871\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "dataloader = DataLoader(emb_train, batch_size=512, shuffle=False)\n",
    "all_features, all_labels = [], []\n",
    "\n",
    "for features, labels in dataloader:\n",
    "    all_features.append(features.numpy())\n",
    "    all_labels.append(labels.numpy())\n",
    "\n",
    "X_train = np.concatenate(all_features, axis=0)\n",
    "y_train = np.concatenate(all_labels, axis=0)\n",
    "\n",
    "# Step 3: Fit a linear classifier (logistic regression)\n",
    "clf = LogisticRegression(max_iter=1000, solver=\"lbfgs\", multi_class=\"multinomial\")\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Optionally evaluate on a validation/test set\n",
    "val_dataloader = DataLoader(emb_validation, batch_size=512, shuffle=False)\n",
    "\n",
    "val_features, val_labels = [], []\n",
    "for features, labels in val_dataloader:\n",
    "    val_features.append(features.numpy())\n",
    "    val_labels.append(labels.numpy())\n",
    "\n",
    "X_val = np.concatenate(val_features, axis=0)\n",
    "y_val = np.concatenate(val_labels, axis=0)\n",
    "\n",
    "y_pred = clf.predict(X_val)\n",
    "acc = accuracy_score(y_val, y_pred)\n",
    "print(f\"Linear probe accuracy: {acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c332f553-12bf-4780-8afa-52e85e2b4c61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0a173411-0ac2-4931-afe1-2fe822dbe12a",
   "metadata": {},
   "source": [
    "## UNI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ed26d62-d001-41fd-a93e-dd447c7f49e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import timm\n",
    "from timm.data import resolve_data_config\n",
    "from timm.data.transforms_factory import create_transform\n",
    "\n",
    "model = timm.create_model(\"hf-hub:MahmoodLab/uni\", pretrained=True, init_values=1e-5, dynamic_img_size=True)\n",
    "transform = create_transform(**resolve_data_config(model.pretrained_cfg, model=model))\n",
    "model.eval()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63624230-9776-4fab-85b8-214ce5d22ac1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "303.350784"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([p.numel() for p in model.parameters()]) / 1e6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3aa97373-ebb7-4a3b-8454-ffec06d9d816",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torch.utils import data\n",
    "\n",
    "class CustomDataset(data.Dataset):\n",
    "\n",
    "    def __init__(self, data, transform=None):\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img, label = self.data[index]\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return (img, label)\n",
    "\n",
    "\n",
    "class EmbeddingDataset(data.Dataset):\n",
    "    def __init__(self, dataset, model, transform, device):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.embeddings, self.labels = self._create_vectors(model, dataset)\n",
    "\n",
    "    def _create_vectors(self, model, dataset):\n",
    "        embeddings = []\n",
    "        label_list = []\n",
    "        model.to(self.device)\n",
    "        dataloader = data.DataLoader(dataset, batch_size=8)\n",
    "\n",
    "        # loop over the data\n",
    "        for batch in tqdm(dataloader):\n",
    "            images, labels = batch\n",
    "            images = images.to(self.device)\n",
    "\n",
    "            # make the embeddings from the batch\n",
    "            with torch.no_grad():\n",
    "                embs = model(images).to(\"cpu\")\n",
    "            \n",
    "            embeddings.append(embs)\n",
    "            label_list.append(labels)\n",
    "\n",
    "\n",
    "        embeddings = torch.cat(embeddings, dim=0)\n",
    "        label_list = torch.cat(label_list, dim=0)\n",
    "        return embeddings, label_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.embeddings)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.embeddings[idx, :], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "63bc0be7-d384-4136-9f12-cc629b670c33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1495/1495 [02:23<00:00, 10.41it/s]\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda:6\"\n",
    "training_dataset = CustomDataset(train_dataset, transform)\n",
    "emb_train = EmbeddingDataset(training_dataset, model, transform, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5f30caba-e01a-416b-bc7c-12fda273aa31",
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = emb_train[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "14fb1a00-74ba-4759-a13b-c724e1a1765c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import resnet18\n",
    "\n",
    "cnn = resnet18(num_classes=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189a0d4e-ccc0-4142-915c-38f7ef2a34a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
