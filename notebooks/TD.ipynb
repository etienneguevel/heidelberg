{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e54e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3948fd75",
   "metadata": {},
   "source": [
    "# Heidelberg Workshop number 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32632536",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/etienneguevel/heidelberg/blob/main/notebooks/TD.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2ccd1d",
   "metadata": {},
   "source": [
    "##  Setup\n",
    "If running locally using jupyter, first intall the necessary libraries in your environment using the installation instructions in the repository.\n",
    "\n",
    "If running from Google Colab, set `using_colab=True` below and run the cell.\n",
    "In Colab, be sure to select 'GPU' under 'Resources'->'Modify the type of execution'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27428f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "using_colab = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9755df",
   "metadata": {},
   "outputs": [],
   "source": [
    "if using_colab:\n",
    "    import torch\n",
    "    import torchvision\n",
    "    print(\"PyTorch version:\", torch.__version__)\n",
    "    print(\"Torchvision version:\", torchvision.__version__)\n",
    "    print(\"CUDA is available:\", torch.cuda.is_available())\n",
    "    import sys\n",
    "    !git clone https://github.com/etienneguevel/heidelberg.git\n",
    "    !{sys.executable} -m pip install -q -r ./heidelberg/requirements_colab.txt\n",
    "    !{sys.executable} -m pip install --no-deps ./heidelberg/\n",
    "    %cd heidelberg/notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deedb402",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "from medmnist import BloodMNIST\n",
    "\n",
    "# We need to download the dataset it might take a while\n",
    "train_dataset = BloodMNIST(split=\"train\", size=64, download=True)\n",
    "val_dataset = BloodMNIST(split=\"val\", size=64, download=True)\n",
    "test_dataset = BloodMNIST(split=\"test\", size=64, download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c516524e-e542-4874-af96-c19ab0f3b5ec",
   "metadata": {},
   "source": [
    "## Classification\n",
    "\n",
    "Classification is a key task as the relative proportion of the different white blood cells\n",
    "categories indicates the presence or not of a pathology.\n",
    "\n",
    "This part's goal is to leverage an open-source dataset of white blood cells in order to\n",
    "train a Deep Learning model to perform classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c51b90b-5e4a-4dea-b9cc-4d7bb00ebbd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's load the dataset\n",
    "train_dataset.info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318af3cf-4f41-410b-8d0c-7ede38f4b7c3",
   "metadata": {},
   "source": [
    "This dataset contains images of white blood cells that are divided between 8 categories:\n",
    "\n",
    "- basophil\n",
    "- eosinophil\n",
    "- erythroblast\n",
    "- immature granulocytes\n",
    "- lymphocyte\n",
    "- monocyte\n",
    "- neutrophil\n",
    "- platelet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d31803-0d3d-4f62-8dce-c788255adb7a",
   "metadata": {},
   "source": [
    "### Understand an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1bd82d-5eaf-48d7-927c-7593444b0dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_size = len(train_dataset)\n",
    "\n",
    "label_dict = train_dataset.info[\"label\"]\n",
    "img, label = train_dataset[random.randint(0, train_size)]\n",
    "\n",
    "print(f\"Image's class is: {label_dict.get(str(label[0]))}\\n\")\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb6eb8e",
   "metadata": {},
   "source": [
    "The images are made of pixels, made of a grid of size H*L, and each pixel is made\n",
    "of 3 channels : Red, Green and Blue.  \n",
    "\n",
    "This makes images **3-Dimensional** objects (N\\*L\\*3), here the images are of size\n",
    "64\\*64\\*3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a26c207-15d2-48e3-aa94-1b759e030143",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Convert the image to a numpy array\n",
    "image = np.array(img.convert(\"RGB\"))\n",
    "print(f\"Image shape: {image.shape}\")\n",
    "\n",
    "# Plot the RGB channels of the image separately\n",
    "figure, plots = plt.subplots(ncols=3, nrows=1, figsize=(12, 4))\n",
    "for i, subplot in zip(range(3), plots):\n",
    "    temp = np.zeros(image.shape, dtype='uint8')\n",
    "    temp[:,:,i] = image[:,:,i]\n",
    "    subplot.imshow(temp)\n",
    "    subplot.set_axis_off()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1744f934-0a37-4e70-8c6a-c89374c3eb7d",
   "metadata": {},
   "source": [
    "### Create the data objects\n",
    "\n",
    "`torch` is a popular python framework to make Deep Learning models. Among its functionalities, it offers ways\n",
    "to facilitate data usage through `Dataset` and `DataLoader` objects.  \n",
    "The first step is to create a `Dataset` ([here](https://docs.pytorch.org/tutorials/beginner/basics/data_tutorial.html)) object.  \n",
    "\n",
    "You should implement three methods for that :\n",
    "- `__init__` -> create the 'attributes' of our object (kind of a way of storing data)\n",
    "- `__len__` -> return the number of elements of our dataset (allows our object to get called with the `len` function)\n",
    "- `__getitem__` -> return the element at an index (like for a `list`). In our case an item is made of an image transformed into a `tensor` + its label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080cd7fc-8dc9-4d91-ac4d-7f41d887b9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first step is to make a dataset, for this we need to create our own custom object\n",
    "# Below is the \"backbone\" of a Dataset object, with all the necessary methods that need to be implemented\n",
    "# Uncomment and execute the cell below to see the answer\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data, transform):\n",
    "        # initialization method, you should store data and transform as attributes\n",
    "        pass\n",
    "\n",
    "    def __len__(self):\n",
    "        # should return the number of elements of the dataset\n",
    "        return\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # should return the element of the dataset at index idx (image and label)\n",
    "        # (Don't forget to transform the image!)\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef43971f-c65e-46db-8151-38ce5a6a98e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../src/custom_dataset.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30ddaaf-a8fe-4b71-aaab-7649e840b376",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Here we need a transform function to convert the data which are PIL images into vectors\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "# We use the custom dataset we just created with our imported datasets\n",
    "training_dataset = CustomDataset(train_dataset, transform)\n",
    "validation_dataset = CustomDataset(val_dataset, transform)\n",
    "testing_dataset = CustomDataset(test_dataset, transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f22c54e-597c-44d3-98bc-24878b49950c",
   "metadata": {},
   "source": [
    "What we will want to do afterwards is to loop over the dataset, to make 'batches'\n",
    "of data for the training protocol.   \n",
    "\n",
    "`torch` provides the `DataLoader` object for this, let's use it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aec8ab9-9377-4792-bed7-e808dd6cb2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a DataLoader from the custom dataset created above\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(training_dataset, batch_size=64)\n",
    "valid_loader = DataLoader(validation_dataset, batch_size=64)\n",
    "test_loader = DataLoader(testing_dataset, batch_size=64)\n",
    "\n",
    "# Let's check that the dataloader works\n",
    "for images, labels in train_loader:\n",
    "    print(f\"Batch size: {images.shape[0]}\")\n",
    "    print(f\"Image shape: {images.shape[1:]}\")\n",
    "    print(f\"Labels shape: {labels.shape}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b0f235-69ef-4332-8050-09c86e3d180c",
   "metadata": {},
   "source": [
    "### Create the model\n",
    "\n",
    "The model we are going to use for this task are Convolutional Neural Networks\n",
    "([CNN](https://poloclub.github.io/cnn-explainer/)), and especially a family\n",
    "of them called [ResNets](https://arxiv.org/abs/1512.03385).  \n",
    "\n",
    "![cnn](./images/cnn.jpg)\n",
    "\n",
    "CNN uses **Convolution** layers, the next figure shows how the convolution is\n",
    "performed when having three input channels.  \n",
    "First, one filter uses its three independent kernels to convolve with the RGB\n",
    "channels of the input image:\n",
    "\n",
    "![convolution](https://cdn-images-1.medium.com/max/1000/1*8dx6nxpUh2JqvYWPadTwMQ.gif)\n",
    "\n",
    "that are then summed to obtain a single channel.\n",
    "\n",
    "![sum](https://cdn-images-1.medium.com/max/1000/1*CYB2dyR3EhFs1xNLK8ewiA.gif)\n",
    "\n",
    "This process is repeated a certain number of times with different kernels,\n",
    "giving what is called **channels**.\n",
    "\n",
    "> The number of channels does not stay constant within a network, usually it\n",
    "> gradually increases!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a784dd33",
   "metadata": {},
   "source": [
    "Training an efficient model can be time-consuming. However, models for image\n",
    "processing have already been created and trained for similar tasks, and are\n",
    "available for reusage.  \n",
    "\n",
    "Those models are called **pretrained**, and can be use to be adapted on our\n",
    "dataset (fitting them on our custom classes of white blood cells).  \n",
    "This process is called **fine-tuning**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5f0849-827e-4afa-b8d5-0a1e33eb7d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create a model from a pretrained point\n",
    "from torchvision import models\n",
    "\n",
    "model = models.resnet18(pretrained=True)\n",
    "\n",
    "# We can see the blocks in our model like this\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9819f389",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba62424",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# We replace the last layer of the model with an out shape equal to our number of classes \n",
    "n_labels = len(train_dataset.info.get(\"label\"))\n",
    "model.fc = nn.Linear(in_features=512, out_features=n_labels)\n",
    "model.to(\"cpu\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce965b10-4c9b-450c-a85f-6d4949f7c822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can see the number of parameters of our model like this\n",
    "print(f\"There are {sum([p.numel() for p in model.parameters()]):.2g} parameters in the model used.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a162787",
   "metadata": {},
   "outputs": [],
   "source": [
    "for (images, labels) in train_loader:\n",
    "    print(f\"Batch size: {images.shape[0]}\")\n",
    "    print(f\"Image shape: {images.shape[1:]}\")\n",
    "    print(f\"Labels shape: {labels.shape}\")\n",
    "\n",
    "    outputs = model(images)\n",
    "    print(f\"Output shape: {outputs.shape}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b093cf40-3e6a-4e42-9127-c1a74ac5d27a",
   "metadata": {},
   "source": [
    "### Training loop\n",
    "\n",
    "The next step of our process is the actual training of our model, the steps are\n",
    "as follow :\n",
    "\n",
    "- Load a batch of our dataloader\n",
    "- Get the output of our model\n",
    "- Compute the loss with the labels\n",
    "- Update the parameters of our model\n",
    "\n",
    "For this we are going to need the following objects :\n",
    "\n",
    "- A criterion (ie a way to calculate the loss); for multi-label classification,\n",
    "    **cross-entropy** is the most widely used loss\n",
    "- An optimizer (ie an algorithm to update the weights of our model). Several\n",
    "    popular options exists (Adam, Stochastic Gradient Descent...)\n",
    "\n",
    "\n",
    "Then we will implement the training loop and make the model train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0ff167-f9d2-41fa-a0bc-bc8d2e3f3492",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we need to make an optimizer + a loss\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam, SGD, AdamW\n",
    "\n",
    "# cross-entropy loss is the one used for multi classification tasks\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Adam is a popular optimizer, but other could be used (SGD, Adamw...)\n",
    "optimizer = Adam(model.parameters(), lr=0.001, weight_decay=1e-4) # you can also use SGD or AdamW, or try different learning rates and weight decay values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197775a2-dcff-42b0-a97c-0f6bd8f4c52c",
   "metadata": {},
   "source": [
    "Now we have everything necessary in order to launch the training process!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77081949",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement the training loop here\n",
    "# Uncomment and execute the cell below to see the answer\n",
    "from tqdm import tqdm # tqdm is a library to display progress bars while looping\n",
    "\n",
    "# Move the model to the device (CPU or GPU)\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Training loop\n",
    "n_epochs = 10 # Number of epochs to train the model (ie number of times the model will see the whole dataset)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "\n",
    "    for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{n_epochs}\"):\n",
    "        optimizer.zero_grad()  # Zero the gradients -> essential step before the backward pass\n",
    "\n",
    "        # ToDo: Calculate the outputs of the model\n",
    "        \n",
    "        # ToDo: Compute the loss\n",
    "        loss = 0\n",
    "\n",
    "        # Make the backward pass and update the weights\n",
    "        loss.backward()  # Backward pass\n",
    "        optimizer.step()  # Update the weights\n",
    "\n",
    "    # Bonus: Implement the validation loop here\n",
    "    # You need to set the model to evaluation mode and use the validation dataloader\n",
    "    # to compute the validation loss and accuracy of the model every k epochs (e.g. every 5 epochs)\n",
    "    if epoch % 5 == 0:\n",
    "        model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e32824-3dea-4557-bfd9-9ca6682e177b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../src/training.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acdacc47-cfbd-42f1-bfba-e701011036e8",
   "metadata": {},
   "source": [
    "### Visualize the results\n",
    "\n",
    "Our dataset was split into three parts: train ,val and test. While we have used\n",
    "the train for the backpropagation, and the val for monitoring, test is still\n",
    "unseen to this point.\n",
    "\n",
    "It's purpose is to the metrics at the endpoint of the training pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f817fef4-2ae7-473c-b893-2809345d7ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute this cell to see the results of your trained model on the test dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "def find_accuracy(model, dataloader_test, device):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    label_test = []\n",
    "    predicted_test = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(dataloader_test):\n",
    "            # Move the data to the device \n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Predict the label with the trained model\n",
    "            outputs = model(inputs)\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            \n",
    "            # Calculate the performance\n",
    "            labels = labels.squeeze(1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "            # Add the predictions & labels in the list\n",
    "            label_test.extend(labels.to('cpu').tolist())\n",
    "            predicted_test.extend(preds.to('cpu').tolist())\n",
    "\n",
    "    print('Accuracy of the network on the test images: %d %%' % (\n",
    "        100 * correct / total))\n",
    "    return predicted_test, label_test, correct / total\n",
    "\n",
    "# Specify the device\n",
    "device = torch.device(\"cuda\")\n",
    "model.to(device)\n",
    "\n",
    "# Calculate the results\n",
    "predictions, labels, acc = find_accuracy(model, test_loader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05db7704",
   "metadata": {},
   "source": [
    "In classification problems, it is also important to check that the algorithm\n",
    "has good performances on every classes, and not only on the most dominants.  \n",
    "\n",
    "Four metrics are usually looked at when we deal with classification issues:\n",
    "- **Accuracy**, which represents the percentage of correct predictions\n",
    "- **Positive Predictive Value (ppv)**, which represents the number of correct\n",
    "    predictions among the total predictions made for a class  \n",
    "    (ie, how much can i trust my model if it predicts that a sample is of this\n",
    "    class)\n",
    "- **Recall**, which represents the number of correct predictions among all the\n",
    "    samples of this class in the test dataset.  \n",
    "    (ie, if I have a sample of a class how much can i trust my model to find it)\n",
    "- **f1-score**, which is calculated as the harmonic mean of ppv and recall.\n",
    "    It represents a compromise between those two objectives.\n",
    "\n",
    "Indeed, in case of imbalanced dataset, training can be biased and less populated\n",
    "classes have deteriorated performances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85bccbd9-a0e1-4317-947f-50292d10979a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute this cell to visualize the results of your model on the test set\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "names = [n[:20] for n in train_dataset.info.get(\"label\").values()]\n",
    "print(classification_report(labels, predictions, target_names=names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8010e926",
   "metadata": {},
   "source": [
    "Another way to visualize the results is with a confusion matrix. It is a table\n",
    "layout where each row represents a class, and each column represents the\n",
    "instances in a predicted class.  \n",
    "\n",
    "![c_matrix](./images/confusion_matrix.png)\n",
    "\n",
    "*Here is an example of a confusion matrix with voyels as class*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9ba7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "names = [n[:20] for n in train_dataset.info.get(\"label\").values()]\n",
    "\n",
    "C_matrix = pd.DataFrame(confusion_matrix(labels, predictions))\n",
    "C_matrix.index = names\n",
    "C_matrix.columns = names\n",
    "\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(C_matrix, annot=True, cmap=\"flare\", vmax=100, fmt='.3g')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9f8d86",
   "metadata": {},
   "source": [
    "### Bonus : Make your own Network\n",
    "\n",
    "Just before we trained a model, but it was a pretrained one, and we did not\n",
    "pick its different layers.  \n",
    "\n",
    "Pytorch's framework allows to easily create your model with the `nn.Module` class.  \n",
    "The idea is to :\n",
    "- create the layers of our model within the `__init__` method. torch already has\n",
    "    all of the most-used layers implemented,  \n",
    "    you just need to pick the ones interesting you !  \n",
    "\n",
    "- Implement a `forward` method. This method will be the one used when calling\n",
    "    `model(inputs)`, and return the outputs of the model.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53cf4d50",
   "metadata": {},
   "source": [
    "> **Tip** : the `torch.nn.Sequential` object allows to concatenate different torch\n",
    "> layers together, as an example a CNN layer could be like :\n",
    ">\n",
    ">```Python\n",
    ">import torch.nn as nn\n",
    ">\n",
    ">nn.Sequential(\n",
    ">    nn.Conv2d(in_channels=32, out_channels=32, kernel_size=5, padding='same'),\n",
    ">    nn.BatchNorm2d(32),\n",
    ">    nn.ReLU(),\n",
    ">    nn.MaxPool2d(kernel_size=(2, 2))\n",
    ">)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f18470",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bonus: You can also make your own model architecture!\n",
    "# Uncomment and execute the cell below to see a simple CNN appear\n",
    "import torch.nn as nn\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        super(Net, self).__init__()\n",
    "        # create layers here\n",
    "        # self.l1 = ...\n",
    "\n",
    "    def forward(self, x):\n",
    "        # here the output should be a vector of size num_classes\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f642f55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../src/net.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73276e14-bd1c-4b36-8fd6-0336db194216",
   "metadata": {},
   "source": [
    "## Foundation models\n",
    "\n",
    "*\"Foundation models\"* are called like this because their outputs, called **embeddings**\n",
    "are not predictions but vectors of a shape $\\mathbb{R}^n$.  \n",
    "The embeddings are the angular stone used for other usages, which can be classification,\n",
    "segmentation, multimodal models..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abad27d9-904e-494e-86d7-bb8cc8ec2cf0",
   "metadata": {},
   "source": [
    "In theory every architecture can be used to make a fondation model, but in practice\n",
    "[Transformers](https://poloclub.github.io/transformer-explainer/) are THE type of\n",
    "model commonly used,  \n",
    "and have been the workhorse of the AI ecosystem for the last 8 years."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0ef1f8-1f0e-406d-982a-5ed57e7e1432",
   "metadata": {},
   "source": [
    "### Vision Transformers (ViT)\n",
    "\n",
    "[ViTs](https://arxiv.org/abs/2010.11929) are the adaptation of transformers for images; they take the image,\n",
    "split them in squares that are transformed in vectors (called **embeddings**),  \n",
    "and then pass them through a Transformer Neural Network.  \n",
    "\n",
    "![ViT_tokens](./images/ViT_token.png)\n",
    "\n",
    "A special token, named **[CLS]** token, is added to the sequence of tokens,\n",
    "whose state at the output of the model serves as an **image representation**.  \n",
    "It is then used for the downstream tasks, like classification.  \n",
    "> The [CLS] token is an artificial construction; it doesn't represent anything at the beginning, but his job is to concatenate the\n",
    "information of the image patches into one **embedding**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c91287-0eae-4c02-affb-e7ec782ae4df",
   "metadata": {},
   "source": [
    "The [model](https://arxiv.org/abs/2404.05022) we are going to used in this part has been\n",
    "trained on White Blood Cells images, it:\n",
    "- Uses the Vision Transformer ([ViT](https://arxiv.org/abs/2010.11929)) architecture\n",
    "- Uses [DINOv2](https://github.com/facebookresearch/dinov2) as training framework\n",
    "- Was trained on ~300k images from open-sourced datasets\n",
    "- Contains 4 models of different sizes, ranging from 22M to 1.1B of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a45eb0c-eb40-470d-8e06-d9915601240d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "\n",
    "# Load model from the hub\n",
    "model = timm.create_model(\n",
    "  model_name=\"hf-hub:1aurent/vit_small_patch14_224.dinobloom\", # you can change the size of the loaded model here\n",
    "  pretrained=True,\n",
    ").eval()\n",
    "\n",
    "# Get model specific transforms (normalization, resize)\n",
    "data_config = timm.data.resolve_model_data_config(model)\n",
    "transform = timm.data.create_transform(**data_config, is_training=False)\n",
    "\n",
    "print(\"Below is the list of the layers contained within our model:\\n\")\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c6f547-5a87-4212-a6b7-be14ada7ddb8",
   "metadata": {},
   "source": [
    "Here we can see the architecture of the model that we use, which is very long.\n",
    "\n",
    "Indeed it is made of 12 blocks, each having:\n",
    "- An Attention layer\n",
    "- A Mlp layer with one hidden layer\n",
    "- Normalizing and Scaling layers in between\n",
    "\n",
    "> Can you find the dimension of the embeddings of the model that you use ? What about its number of elements ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2674702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try here, or uncomment and execute the cell below to see the answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec77f65-ef82-40bf-84ee-a6497e639a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../src/model_information.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e83140-1ed9-4085-a10c-b19d46e05068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's visualize a random image from the training set and use the model on it\n",
    "img, label = train_dataset[random.randint(0, train_size)]\n",
    "\n",
    "print(f\"Image's class is: {label_dict.get(str(label[0]))}\\n\")\n",
    "plt.imshow(img)\n",
    "plt.show()\n",
    "# Below is the code to use the model on a single image\n",
    "\n",
    "data = transform(img).unsqueeze(0) # input is a (batch_size, num_channels, img_size, img_size) shaped tensor\n",
    "output = model(data)\n",
    "\n",
    "print(f\"Look of the output:\\n{output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74899774-3d2c-4575-a8bf-3f5ee5810993",
   "metadata": {},
   "source": [
    "### Probe the embeddings quality\n",
    "\n",
    "Now that we have an easy access to the model embeddings, we can test their quality on the dataset\n",
    "that we have used before.\n",
    "\n",
    "Popular technics are:\n",
    "- k-[Nearest-Neighbour](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier) (k-  NN), with k being usually 1 & 20, which consists on making a prediction according\n",
    "    to the most similar points of the training dataset  \n",
    "    (k being the number of neighbours to take into account).\n",
    "\n",
    "![NN](./images/knn.png)\n",
    "\n",
    "- [Linear Probing](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html),\n",
    "    which consists of fitting a linear regression on the embeddings, and then\n",
    "    evaluating its performance.\n",
    "\n",
    "![linear_probing](./images/Multiclass-logistic-regression-classification.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ac339f-da81-40b8-ad10-1ba3d2a295b5",
   "metadata": {},
   "source": [
    "#### Make the dataset\n",
    "\n",
    "Calculating the embeddings can be compute intensive, if we use transformer models\n",
    "having huge amounts of parameters.  \n",
    "However we only need to do that once, as we use them as mathematical objects, and\n",
    "**we do not modify** the foumdation model afterwards.  \n",
    "\n",
    "To do that we will create a Dataset object that will contain the embeddings calculated\n",
    "for a model on a Dataset (the same as the one we used before)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a230ecd3-9e82-4bb8-971b-49eb7a0956ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are going to make a new dataset object in order to calculate the embeddings of the images.\n",
    "# This allow to calculate once the embeddings and then reuse them for later applications\n",
    "from heidelberg.embedding_dataset import EmbeddingDataset\n",
    "\n",
    "# Make the embedding dataset and test shapes\n",
    "emb_train = EmbeddingDataset(training_dataset, model, transform, n_samples=1600)\n",
    "emb_test = EmbeddingDataset(testing_dataset, model, transform)\n",
    "\n",
    "# Unpack the embeddings & labels\n",
    "train_array = np.array([emb for emb, _ in emb_train])\n",
    "train_labels = np.array([lab for _, lab in emb_train])\n",
    "\n",
    "test_array = np.array([emb for emb, _ in emb_test])\n",
    "test_labels = np.array([lab for _, lab in emb_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b252d016-ddec-45fc-847f-3935ef85a41e",
   "metadata": {},
   "source": [
    "Implement here the evaluation of the above mentionned techniques!  \n",
    "\n",
    "> Tip: You can use the sklearn library for k-NN and Linear probing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48bee05-75e0-4fdf-9b42-3eced1c1c110",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to make function implementing the k-NN and linear probing of the embedding\n",
    "# Uncomment and execute the cell below to get the answer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "\n",
    "names = [n[:20] for n in train_dataset.info.get(\"label\").values()]\n",
    "\n",
    "def k_nearest_neighbor_eval(\n",
    "    train_array,\n",
    "    train_labels,\n",
    "    test_array,\n",
    "    test_labels,\n",
    "    k=1,\n",
    "    target_names=names\n",
    "):\n",
    "    \"\"\"\n",
    "    Define a function that train a k-NN classifier on the training embeddings\n",
    "    and evaluates it on the validation embeddings.\n",
    "    \"\"\"\n",
    "    # ToDo: Initialize the classifier\n",
    "\n",
    "    \n",
    "    # ToDo: Fit the model on train_array and train_labels\n",
    "    \n",
    "\n",
    "    # ToDo: Make the predictions on test_array\n",
    "    preds = None  # Replace with your predictions\n",
    "\n",
    "    # Calculate the metrics of the predictions\n",
    "    print(classification_report(test_labels, preds, target_names=target_names))\n",
    "\n",
    "    return\n",
    "\n",
    "\n",
    "def linear_probing_eval(\n",
    "    train_array,\n",
    "    train_labels,\n",
    "    test_array,\n",
    "    test_labels,\n",
    "    target_names=names\n",
    "):\n",
    "    \"\"\"\n",
    "    Define a function that train a k-NN classifier on the training embeddings\n",
    "    and evaluates it on the validation embeddings.\n",
    "    \"\"\"\n",
    "    # ToDo: Initialize the classifier\n",
    "\n",
    "\n",
    "    # ToDo: Fit the model on train_array and train_labels\n",
    "    \n",
    "\n",
    "    # ToDo: Make the predictions on test_array\n",
    "    preds = None  # Replace with your predictions\n",
    "\n",
    "    # Calculate the metrics of the predictions\n",
    "    print(classification_report(test_labels, preds, target_names=target_names))\n",
    "\n",
    "    return\n",
    "\n",
    "\n",
    "# Use the functions defined above to evaluate the embeddings\n",
    "print('1-NN evaluation:\\n')\n",
    "_ = k_nearest_neighbor_eval(\n",
    "    train_array,\n",
    "    train_labels,\n",
    "    test_array,\n",
    "    test_labels,\n",
    "    k=1\n",
    ")\n",
    "print('-' * 75)\n",
    "\n",
    "print('\\n20-NN evaluation:\\n')\n",
    "_ = k_nearest_neighbor_eval(\n",
    "    train_array,\n",
    "    train_labels,\n",
    "    test_array,\n",
    "    test_labels,\n",
    "    k=20\n",
    ")\n",
    "print('-' * 75)\n",
    "\n",
    "print('\\nLinear probing:\\n')\n",
    "_ = linear_probing_eval(\n",
    "    train_array,\n",
    "    train_labels,\n",
    "    test_array,\n",
    "    test_labels\n",
    ")\n",
    "print('-' * 75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204bf7cf-422f-437d-a025-ffbdd389b0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../src/embedding_evaluation.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332f2687-65a5-401b-8360-69c0a21de557",
   "metadata": {},
   "source": [
    "#### Visualize the embeddings\n",
    "\n",
    "Visualisation of the embeddings in a 2D (x - y) plan is a good way to check the quality of our embeddings.  \n",
    "Indeed, if the embeddings we produced are of a good quality, there should be clusters corresponding to each one\n",
    "of our classes.  \n",
    "\n",
    "Among popular dimensionality reduction techniques there are :\n",
    "- [UMAP](https://umap-learn.readthedocs.io/en/latest/basic_usage.html), a popular method for high dimensional biological datasets that captures well the clusters within a dataset\n",
    "- [t-SNE](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html),\n",
    "    **ToDo**: find a def of t-SNE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb94b41-5a2b-44fc-a4a2-5fa37561d888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try implementing one of the two techniques, and use the plot function below to get the 2D representation\n",
    "# Uncomment the cell below for the answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9884f8c-cb3c-4eba-a524-50e6acc89197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../src/umap_tsne.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfcb213-49e1-44c8-88c9-71b557ad72d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "translate_dict = train_dataset.info.get(\"label\")\n",
    "\n",
    "def plot_embeddings(embs, labels):\n",
    "    \n",
    "    # Make a dataframe with the embeddings\n",
    "    data = pd.DataFrame(\n",
    "        [\n",
    "            [e[0], e[1], translate_dict.get(str(lab[0]))[:20]]\n",
    "            for e, lab in zip(embs, labels)\n",
    "        ],\n",
    "        columns=[\"x\", \"y\", \"class\"]\n",
    "    )\n",
    "\n",
    "    sns.relplot(\n",
    "        data=data,\n",
    "        x=\"x\", y=\"y\", hue=\"class\", style=\"class\", height=8, aspect=1.5\n",
    "    )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9971ea-6b1e-40dd-93ff-6dcf4f81fadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "uemb, labels = get_umap(emb_test)\n",
    "plot_embeddings(uemb, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a82ab67-ab48-484e-895f-0e5d484ff785",
   "metadata": {},
   "outputs": [],
   "source": [
    "temb, labels = get_tsne(emb_test)\n",
    "plot_embeddings(temb, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796482a8-f175-4360-b1d3-2b75d730516c",
   "metadata": {},
   "source": [
    "### Bonus : Attention map\n",
    "\n",
    "As of now we almost only used the [CLS] token, and left alone the tokens of the image patches.  \n",
    "\n",
    "Embeddings of an image evolve from one layer to another, and are calculated through the attention mechanism.  \n",
    "In the attention mechanism new tokens are updated according to their similarity with other tokens (the more similar,\n",
    "the more their update will be consequent).\n",
    "\n",
    "\n",
    "As such, one interesting thing to look at is the **attention map of the [CLS] token in the last layer**,\n",
    "as it kind of indicate which parts of the images are most used for the creation the image's embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2c5540-f3f3-4936-a7f1-3fec4ab5964b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from heidelberg.attn import get_attn\n",
    "\n",
    "patch_h, patch_w = 37, 37\n",
    "model.cpu()\n",
    "\n",
    "def plot_attention_map(img, layer, model, save=False):\n",
    "    img_tensor = transform(img)\n",
    "    cls_attn = get_attn(img_tensor, model, layer)\n",
    "    num_heads, _ = cls_attn.shape\n",
    "    \n",
    "    cls_tot = torch.sum(cls_attn, dim=0).reshape((patch_h, patch_w))\n",
    "    \n",
    "    img_or = np.array(img) \n",
    "    # Plot the total attention (summed over the heads)\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(8, 4))\n",
    "    \n",
    "    ax[0].imshow(img_or)\n",
    "    ax[0].axis(\"off\")\n",
    "    ax[0].set_title(\"Original image\")\n",
    "    ax[1].imshow(cls_tot)\n",
    "    ax[1].axis(\"off\")\n",
    "    ax[1].set_title(\"Attention map <cls> token\")\n",
    "    \n",
    "    if save:\n",
    "        plt.savefig(\"attention_map_cls.png\")\n",
    "    plt.show()\n",
    "\n",
    "    # Plot the attention of the heads\n",
    "    k, l = 3, num_heads // 3 + (num_heads % 3 != 0)\n",
    "    fig, ax = plt.subplots(l, k, figsize=(8, 4))\n",
    "\n",
    "    for i in range(l):\n",
    "        for j in range(k):\n",
    "            attn_map = cls_attn[3 * i + j, :].reshape((patch_h, patch_w))\n",
    "            ax[i][j].imshow(attn_map)\n",
    "            ax[i][j].axis(\"off\")\n",
    "            ax[i][j].set_title(f\"Attention map of Head {3 * i + j}\")\n",
    "    plt.show()\n",
    "\n",
    "last_layer = [blk for blk in model.blocks][-1]\n",
    "plot_attention_map(img, last_layer, model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
